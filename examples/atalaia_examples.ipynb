{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Atalaia\n",
    "\n",
    "Atalaia is a collection of methods that can be used for simple NLP tasks. It can be used for tasks involving text preprocessing for machine learning. Atalaia works mainly with text strings and lists of strings. In order to use it, import the module and define the instances with the languages you need.\n",
    "\n",
    "You can use Atalaia with the following languages (but not every feature is available for all of them):\n",
    "\n",
    "- pt-br\n",
    "- en\n",
    "- fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Atalaia\n",
    "from atalaia.atalaia import Atalaia\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# starting Atalaia instances\n",
    "atalaia_pt_br = Atalaia('pt-br')\n",
    "atalaia_en = Atalaia('en')\n",
    "atalaia_fr = Atalaia('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by doing some text removal/replacing preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# removing html tags\n",
    "removed_html = atalaia_en.remove_html_tags('<h1><strong>Hello world!</strong></h1>')\n",
    "print(removed_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML HTML Hello world! HTML HTML\n"
     ]
    }
   ],
   "source": [
    "# if you want, you can use a placeholder to mark the locations where the tags where found\n",
    "replaced_html = atalaia_en.replace_html_tags('<h1><strong>Hello world!</strong></h1>', 'HTML')\n",
    "print(replaced_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can go to the page to see the content.\n"
     ]
    }
   ],
   "source": [
    "# remove urls from text...\n",
    "removed_urls = atalaia_en.remove_urls('You can go to the page http://homepage.com to see the content.')\n",
    "print(removed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can go to the page URL to see the content.\n"
     ]
    }
   ],
   "source": [
    "# ...or simply replace them\n",
    "replaced_urls = atalaia_en.replace_urls('You can go to the page http://homepage.com to see the content.', 'URL')\n",
    "print(replaced_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish you all and !\n"
     ]
    }
   ],
   "source": [
    "# remove hashtags\n",
    "removed_hashtags = atalaia_en.remove_hashtags('I wish you all #love and #peace!')\n",
    "print(removed_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish you all HASHTAG and HASHTAG\n"
     ]
    }
   ],
   "source": [
    "# replace the hashtags\n",
    "replaced_hashtags = atalaia_en.replace_hashtags('I wish you all #love and #peace', 'HASHTAG')\n",
    "print(replaced_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This can be accessed on the address .\n"
     ]
    }
   ],
   "source": [
    "# remove ips\n",
    "removed_ips = atalaia_en.remove_ips('This can be accessed on the address 198.162.0.1.')\n",
    "print(removed_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This can be accessed on the address IP.\n"
     ]
    }
   ],
   "source": [
    "# replace ips\n",
    "replaced_ips = atalaia_en.replace_ips('This can be accessed on the address 198.162.0.1.', 'IP')\n",
    "print(replaced_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you come tonight, ?\n"
     ]
    }
   ],
   "source": [
    "# remove @handlers\n",
    "removed_handles = atalaia_en.remove_handles('Can you come tonight, @alice?')\n",
    "print(removed_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you come tonight, USERNAME?\n"
     ]
    }
   ],
   "source": [
    "# or replace them\n",
    "replaced_handles = atalaia_en.replace_handles('Can you come tonight, @alice?', 'USERNAME')\n",
    "print(replaced_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She told me: QUOTED you are a confident and strong woman QUOTED .\n"
     ]
    }
   ],
   "source": [
    "# replace quotes\n",
    "replaced_quotes = atalaia_en.replace_quotes('She told me: \"you are a confident and strong woman\".', 'QUOTED')\n",
    "print(replaced_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I told him ,,, times that he could not do that!\n"
     ]
    }
   ],
   "source": [
    "# remove numbers\n",
    "removed_numbers = atalaia_en.remove_numbers('I told him 1,2,3,4 times that he could not do that!')\n",
    "print(removed_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to replace numbers by words, use the method replace_numbers(). Pay attention to the fact that this WON'T transform complex numbers into a readable form. Eg: 20 will become \"two zero\" and not \"twenty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one is the first number.\n"
     ]
    }
   ],
   "source": [
    "# or replace numbers by words\n",
    "replaced_numbers = atalaia_en.replace_numbers('1 is the first number.')\n",
    "print(replaced_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start to deal with special characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamae me disse: voce e o menino mais agil do time.\n"
     ]
    }
   ],
   "source": [
    "# strip accents\n",
    "stripped_accents = atalaia_pt_br.strip_accents('Mamãe me disse: você é o menino mais ágil do time.')\n",
    "print(stripped_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey are you here I really need your help\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "removed_punctuation = atalaia_en.remove_punctuation('Hey, are you here??? I really need your help!')\n",
    "print(removed_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey PUNCTUATION are you here PUNCTUATION PUNCTUATION PUNCTUATION I really need your help PUNCTUATION\n"
     ]
    }
   ],
   "source": [
    "# replace punctuation\n",
    "replaced_punctuation = atalaia_en.replace_punctuation('Hey, are you here??? I really need your help!', placeholder='PUNCTUATION')\n",
    "print(replaced_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also spot a specific character here. Let's say you want to replace question marks only. You could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you here QUESTION QUESTION QUESTION I really need your help!\n"
     ]
    }
   ],
   "source": [
    "# or replace only a specific char\n",
    "replaced_punctuation = atalaia_en.replace_punctuation('Hey, are you here??? I really need your help!', sign='?', placeholder='QUESTION')\n",
    "print(replaced_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expand current and common contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je veux que on reste ici. Je te embrasse fort.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atalaia_fr.expand_contractions(\"Je veux qu'on reste ici. Je t'embrasse fort.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stem the sentences using the NLTK stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu ador faz compr com minh mã e com minh amig\n",
      "i love to go shop with my mother and friend\n",
      "j'ador fair du shopping avec ma mer et me ami\n"
     ]
    }
   ],
   "source": [
    "stemmed_pt_br = atalaia_pt_br.stem_sentence('Eu adoro fazer compras com minha mãe e com minhas amigas')\n",
    "print(stemmed_pt_br)\n",
    "stemmed_en = atalaia_en.stem_sentence('I love to go shopping with my mother and friends')\n",
    "print(stemmed_en)\n",
    "stemmed_fr = atalaia_fr.stem_sentence('J\\'adore faire du shopping avec ma mère et mes amies')\n",
    "print(stemmed_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adoro fazer compras mãe amigas\n",
      "I love go shopping my mother friends\n",
      "J'adore faire du shopping avec ma mère et mes amies.\n"
     ]
    }
   ],
   "source": [
    "removed_stop_words_pt_br = atalaia_pt_br.remove_stopwords('Eu adoro fazer compras com minha mãe e com minhas amigas')\n",
    "print(removed_stop_words_pt_br)\n",
    "removed_stop_words_en = atalaia_en.remove_stopwords('I love to go shopping with my mother and friends')\n",
    "print(removed_stop_words_en)\n",
    "removed_stop_words_fr = atalaia_fr.remove_stopwords('J\\'adore faire du shopping avec ma mère et mes amies.')\n",
    "print(removed_stop_words_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can provide your own stop words list. To use only your custom list, load Atalaia in custom mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every friday, we eat pizza at my house.\n"
     ]
    }
   ],
   "source": [
    "# use a custom list only, with no language loaded\n",
    "atalaia = Atalaia('custom')\n",
    "custom_stopwords = ['pizza']\n",
    "removed_stop_words_custom = atalaia.remove_stopwords('Every friday, we eat pizza at my house.', custom_list=custom_stopwords)\n",
    "print(removed_stop_words_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to extend the stopwords of Atalaia. Set extend_set to True, while providing a custom stop words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every friday, we eat pizza at my house.\n"
     ]
    }
   ],
   "source": [
    "# use a custom list only while loading a language (extend the set)\n",
    "custom_stopwords = ['pizza']\n",
    "removed_stop_words_en = atalaia_en.remove_stopwords('Every friday, we eat pizza at my house.', custom_list=custom_stopwords, extend_set=True)\n",
    "print(removed_stop_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While preprocessing social media texts, you may find words with repeated chars, like veryyyyyy looooonnnng words. You can use the method reduce_words_with_repeated_chars to normalize them. Be careful: abbreviations like 'AAA' can be interpreted as long texts. Another limitation is that it only catchs words with chars repeated more than 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love pizza so much\n"
     ]
    }
   ],
   "source": [
    "reduce_long_words_en = atalaia_en.reduce_words_with_repeated_chars('I loooooooooooooove pizza so muuuchhhh')\n",
    "print(reduce_long_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the last methods remove parts of the text but replace them by empty spaces. Os sometimes, text come already with these empty spaces that have to be fixed. This can be fixed by the method remove_excessive_spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't stop looking at you\n"
     ]
    }
   ],
   "source": [
    "excessive_spaces_removed = atalaia_en.remove_excessive_spaces('I  can\\'t     stop looking at  you')\n",
    "print(excessive_spaces_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use replace_newline to replace newline char by another char. It will account for sentences finishing with ?.!,:;="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to break free. I want to break free. Do you wanna break free? I do!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atalaia_en.replace_newline('I want to break free.\\nI want to break free\\nDo you wanna break free?\\nI do!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, the default replacement is \". \". \n",
    "\n",
    "You can change it, but if you decide to use another replacement other than punct + space, you have to set consider_punctuation to False. This won't do any modifications to the sentencs that already have punctuation. It will only replace the newline char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to break free.--NEWLINE--I want to break free--NEWLINE--Do you wanna break free?--NEWLINE--I do!'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atalaia_en.replace_newline('I want to break free.\\nI want to break free\\nDo you wanna break free?\\nI do!', \n",
    "                           replacement=\"--NEWLINE--\", \n",
    "                           consider_punctuation = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick preprocessing\n",
    "\n",
    "The preprocess method offers a quick way of preprocessing text. I you call it, it will do the following actions in this order:\n",
    "\n",
    "   - lower text\n",
    "   - strip trailing whitespaces\n",
    "   - convert emojis to text\n",
    "   - replace urls\n",
    "   - remove tags\n",
    "   - replace hashtags\n",
    "   - replace ips\n",
    "   - remove social media handles\n",
    "   - replace numbers\n",
    "   - remove punctuation\n",
    "   - remove excessive spaces     \n",
    "   - tag text if True\n",
    "   - remove stopwords if True\n",
    "   - remove accents\n",
    "   - remove excessive spaces\n",
    "   - stem text if True\n",
    "   - tokenize text (if True, will return a list of tokens)\n",
    "\n",
    "Normal preprocessing will return a list of tokens for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at',\n",
      " 'the',\n",
      " 'end',\n",
      " 'of',\n",
      " 'the',\n",
      " 'day',\n",
      " 'you',\n",
      " 're',\n",
      " 'sole',\n",
      " 'respons',\n",
      " 'for',\n",
      " 'your',\n",
      " 'success',\n",
      " 'and',\n",
      " 'your',\n",
      " 'failur',\n",
      " 'and',\n",
      " 'the',\n",
      " 'sooner',\n",
      " 'you',\n",
      " 'realiz',\n",
      " 'that',\n",
      " 'you',\n",
      " 'accept',\n",
      " 'that',\n",
      " 'and',\n",
      " 'integr',\n",
      " 'that',\n",
      " 'into',\n",
      " 'your',\n",
      " 'work',\n",
      " 'ethic',\n",
      " 'you',\n",
      " 'will',\n",
      " 'start',\n",
      " 'be',\n",
      " 'success',\n",
      " 'as',\n",
      " 'long',\n",
      " 'as',\n",
      " 'you',\n",
      " 'blame',\n",
      " 'other',\n",
      " 'for',\n",
      " 'the',\n",
      " 'reason',\n",
      " 'you',\n",
      " 'aren',\n",
      " 't',\n",
      " 'where',\n",
      " 'you',\n",
      " 'want',\n",
      " 'to',\n",
      " 'be',\n",
      " 'you',\n",
      " 'will',\n",
      " 'alway',\n",
      " 'be',\n",
      " 'a',\n",
      " 'failur']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = atalaia_en.preprocess(\"At the end of the dayyyyyyyy,      you're solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.\")\n",
    "pprint.pprint(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a string instead of a list of tokens, set 'tokenize' to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('at the end of the day you are sole respons for your success and your failur '\n",
      " 'and the sooner you realiz that you accept that and integr that into your '\n",
      " 'work ethic you will start be success as long as you blame other for the '\n",
      " 'reason you aren t where you want to be you will alway be a failur')\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = atalaia_en.preprocess(\"At the end of the dayyyyyyyy, you are solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.\", tokenize=False)\n",
    "pprint.pprint(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is not stemming the words. Just set 'stem' to False too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at',\n",
      " 'end',\n",
      " 'of',\n",
      " 'day',\n",
      " 'you',\n",
      " 'are',\n",
      " 'solely',\n",
      " 'responsible',\n",
      " 'for',\n",
      " 'your',\n",
      " 'success',\n",
      " 'your',\n",
      " 'failure',\n",
      " 'sooner',\n",
      " 'you',\n",
      " 'realize',\n",
      " 'that',\n",
      " 'you',\n",
      " 'accept',\n",
      " 'that',\n",
      " 'integrate',\n",
      " 'that',\n",
      " 'into',\n",
      " 'your',\n",
      " 'work',\n",
      " 'ethic',\n",
      " 'you',\n",
      " 'will',\n",
      " 'start',\n",
      " 'being',\n",
      " 'successful',\n",
      " 'as',\n",
      " 'long',\n",
      " 'as',\n",
      " 'you',\n",
      " 'blame',\n",
      " 'others',\n",
      " 'for',\n",
      " 'reason',\n",
      " 'you',\n",
      " 'aren',\n",
      " 't',\n",
      " 'where',\n",
      " 'you',\n",
      " 'want',\n",
      " 'be',\n",
      " 'you',\n",
      " 'will',\n",
      " 'always',\n",
      " 'be',\n",
      " 'failure']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = atalaia_en.preprocess(\"At the end of the dayyyyyyyy, you are solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.\", stem=False, remove_stopwords=True)\n",
    "pprint.pprint(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide corpus into smaller sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List processing\n",
    "\n",
    "If you have a list of strings, like a pandas series, you can pass it directly to Atalaia for preprocessing. You can also choose if you want the sentences to be tokenized and stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'you'],\n",
      " ['please', 'never', 'leave', 'me', 'alone'],\n",
      " ['if', 'you', 'go', 'i', 'will', 'die'],\n",
      " ['i', 'am', 'watching', 'lot', 'of', 'romantic', 'comedy', 'lately'],\n",
      " ['i', 'have', 'eat', 'icecream']]\n"
     ]
    }
   ],
   "source": [
    "list_of_strings = [\n",
    "    'I love you',\n",
    "    'Please, never leave me alone',\n",
    "    'If you go, I will die',\n",
    "    'I am watching a lot of romantic comedy lately',\n",
    "    'I have to eat icecream'\n",
    "]\n",
    "\n",
    "list_processed = atalaia_en.preprocess_list(list_of_strings, stem=False, remove_stopwords=True)\n",
    "pprint.pprint(list_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Sometimes you need to transform a list of strings in a solely string, which we call here corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('  I love you Please, never leave me alone If you go, I will die I am '\n",
      " 'watching a lot of romantic comedy lately I have to eat icecream')\n"
     ]
    }
   ],
   "source": [
    "# creating corpus from list of strings\n",
    "corpus = atalaia_en.create_corpus(list_of_strings)\n",
    "pprint.pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate the lexical diversity of a string given a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity is of this sentence is of 12.5% compared to corpus.\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "diversity = atalaia_en.lexical_diversity(list_of_strings[0], corpus)\n",
    "print('Diversity is of this sentence is of {}% compared to corpus.'.format(diversity*100))\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipelines\n",
    "\n",
    "If you don't want to use the preprocess function, you can build a pipeline just by concanating the methods above. Pay attention to the order you choose to use. If you want to expand contractions, foir instance, don't use a method that strip accents and special chars before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at',\n",
      " 'end',\n",
      " 'of',\n",
      " 'day',\n",
      " 'PUNCTUATION',\n",
      " 'HANDLE',\n",
      " 'you',\n",
      " 'PUNCTUATION',\n",
      " 're',\n",
      " 'solely',\n",
      " 'responsible',\n",
      " 'for',\n",
      " 'your',\n",
      " 'HASHTAG',\n",
      " 'your',\n",
      " 'HASHTAG',\n",
      " 'PUNCTUATION',\n",
      " 'sooner',\n",
      " 'you',\n",
      " 'realize',\n",
      " 'that',\n",
      " 'PUNCTUATION',\n",
      " 'you',\n",
      " 'accept',\n",
      " 'that',\n",
      " 'PUNCTUATION',\n",
      " 'integrate',\n",
      " 'that',\n",
      " 'into',\n",
      " 'your',\n",
      " 'work',\n",
      " 'ethic',\n",
      " 'PUNCTUATION',\n",
      " 'you',\n",
      " 'will',\n",
      " 'start',\n",
      " 'being',\n",
      " 'HASHTAG',\n",
      " 'PUNCTUATION']\n"
     ]
    }
   ],
   "source": [
    "text = \"At the end of the day, @john you're solely responsible for your #success and your #failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being #successful.\"\n",
    "\n",
    "text = atalaia_en.lower_remove_white(text)\n",
    "text = atalaia_en.replace_handles(text, 'HANDLE')\n",
    "text = atalaia_en.replace_hashtags(text, 'HASHTAG')\n",
    "text = atalaia_en.remove_stopwords(text)\n",
    "text = atalaia_en.replace_punctuation(text, placeholder='PUNCTUATION')\n",
    "text = atalaia_en.tokenize(text)\n",
    "pprint.pprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness tools\n",
    "\n",
    "Sometimes you need to test if a model is performing well or badly due to the data quality or simply due to the model itself. In these cases, you can test randomness. Atalaia offers random_classification and replace_with_blob tools. The first creates random labels for each example on your dataset, while the second creates blob text using the vocabulary on your data set to generate random examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Bad: 1 values\n",
      "Class Good: 4 values\n",
      "['Good', 'Good', 'Good', 'Good', 'Bad']\n"
     ]
    }
   ],
   "source": [
    "texts = ['Oi, tudo bom?', 'Queria ser seu namorado.', 'Me liga, ok?', 'Quando voce chega mesmo?', 'Adoro o seu cachorro.']\n",
    "labels = atalaia_pt_br.random_classification(texts, ['Bad','Good'], balanced=True)\n",
    "pprint.pprint(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mesmo? liga, liga,',\n",
      " 'Quando tudo Quando tudo',\n",
      " 'o mesmo? o',\n",
      " 'Me  Oi, seu',\n",
      " 'cachorro. chega chega Oi,']\n"
     ]
    }
   ],
   "source": [
    "sentences = atalaia_pt_br.replace_with_blob(texts)\n",
    "pprint.pprint(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Atalaia comes with an internal tokenizer. To use it, simply access the tokenize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At',\n",
      " 'the',\n",
      " 'end',\n",
      " 'of',\n",
      " 'the',\n",
      " 'day',\n",
      " ',',\n",
      " \"you're\",\n",
      " 'solely',\n",
      " 'responsible',\n",
      " 'for',\n",
      " 'your',\n",
      " 'success',\n",
      " 'and',\n",
      " 'your',\n",
      " 'failure',\n",
      " '.',\n",
      " 'And',\n",
      " 'the',\n",
      " 'sooner',\n",
      " 'you',\n",
      " 'realize',\n",
      " 'that',\n",
      " ',',\n",
      " 'you',\n",
      " 'accept',\n",
      " 'that',\n",
      " ',',\n",
      " 'and',\n",
      " 'integrate',\n",
      " 'that',\n",
      " 'into',\n",
      " 'your',\n",
      " 'work',\n",
      " 'ethic',\n",
      " ',',\n",
      " 'you',\n",
      " 'will',\n",
      " 'start',\n",
      " 'being',\n",
      " 'successful',\n",
      " '.',\n",
      " 'As',\n",
      " 'long',\n",
      " 'as',\n",
      " 'you',\n",
      " 'blame',\n",
      " 'others',\n",
      " 'for',\n",
      " 'the',\n",
      " 'reason',\n",
      " 'you',\n",
      " \"aren't\",\n",
      " 'where',\n",
      " 'you',\n",
      " 'want',\n",
      " 'to',\n",
      " 'be',\n",
      " ',',\n",
      " 'you',\n",
      " 'will',\n",
      " 'always',\n",
      " 'be',\n",
      " 'a',\n",
      " 'failure',\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = atalaia_en.tokenize(\"At the end of the day, you're solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.\")\n",
    "pprint.pprint(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
